# <center>从数据维度探讨提升KNN算法效率的策略——以IRIS数据集为例</center>
# 小组成员
    编程组：莫阳兵、宋雨昕
    文档组：雒一涵、王梓阳
    讲解组：杨洋、李星



## 引言
在机器学习中，k近邻（KNN）算法是一个简单且有效的分类方法。然而，KNN算法的效率高度依赖于数据的维度和特征数量。本文以IRIS数据集为例，通过不同的特征组合和降维策略，探讨了如何在保证分类准确率的前提下提升KNN算法的效率。实验结果表明，适当的降维和特征选择可以显著减少KNN算法的训练和预测时间，而不会明显降低分类性能。

K近邻（KNN）算法是一种基于实例的学习方法，广泛应用于分类和回归任务。然而，KNN算法的计算复杂度较高，尤其在高维数据集上表现突出。因此，如何通过调整数据维度来提升KNN算法的效率成为一个重要的研究课题。本文以IRIS数据集为例，探讨通过特征选择和降维策略提升KNN算法效率的方法。
## 实验步骤

本文采用了以下几种不同的特征组合和降维策略：

    使用原始数据:
        使用IRIS数据集的全部四个特征进行KNN分类。

    使用部分特征:
        仅使用IRIS数据集前两个特征进行KNN分类。

    特征组合:
        将前两个特征和后两个特征分别求和，形成新的特征组合进行KNN分类。

    PCA降维:
        使用主成分分析（PCA）将数据降维到两个主成分进行KNN分类。

## 实验设置

实验过程包括以下步骤：

    数据预处理:
        加载IRIS数据集，并将其分割为训练集和测试集。
        对数据进行标准化处理，使其均值为0，方差为1。

    特征组合和降维:
        根据上述四种策略生成不同的数据集。

    KNN分类器:
        使用KNN分类器（k=3，algorithm='brute'）进行训练和预测。
        记录训练时间、预测时间和分类准确率。

    实验重复:
        每种策略下的实验重复300次，取其平均值进行比较。

## 实验结果

实验结果包括以下几方面的平均值：训练时间、预测时间、总时间和分类准确率。以下是实验结果的汇总：

    原始数据:
    平均训练时间: 0.211027530700 毫秒
    平均预测时间: 0.234669756300 毫秒
    平均运行总时间: 0.445697287000 毫秒
    平均预测准确率: 94.504222222222%

    只取前两个特征的数据:
    平均训练时间: 0.210317203700 毫秒
    平均预测时间: 0.223670183100 毫秒
    平均运行总时间: 0.433987386800 毫秒
    平均预测准确率: 74.317777777778%

    前后两个特征分别求和的数据:
    平均训练时间: 0.209670099400 毫秒
    平均预测时间: 0.220236485800 毫秒
    平均运行总时间: 0.429906585200 毫秒
    平均预测准确率: 97.040000000000%

    PCA数据:
    平均训练时间: 0.209370174600 毫秒
    平均预测时间: 0.219494474600 毫秒
    平均运行总时间: 0.428864649200 毫秒
    平均预测准确率: 95.182222222222%


![分类决策边界和数据点的颜色映射图](.\IMG\DATA.png)  
<center>分类决策边界和数据点的颜色映射图</center>

<br><br>



![最终结果](.\IMG\TimeandAccuracy.png)  
<center>最终结果</center>

## PCA

主成分分析（PCA）

主成分分析（Principal Component Analysis, PCA）是一种常用的降维技术，通过将原始数据集投影到一个新的坐标系中，以最大化数据的方差并减少维度。PCA的主要目标是找到数据中最重要的特征（称为主成分），以便在降低数据维度的同时保留尽可能多的原始信息。以下是PCA的详细介绍：
PCA的基本步骤

    数据标准化：
        将数据标准化是PCA的第一步。标准化的目的是将所有特征的均值调整为0，方差调整为1，以消除特征量纲的影响。标准化公式为： [ X_{standard} = \frac{X - \mu}{\sigma} ] 其中，( X ) 为原始数据，( \mu ) 为均值，( \sigma ) 为标准差。

    计算协方差矩阵：
        协方差矩阵用于描述特征之间的线性关系，计算公式为： [ Cov(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(X_i - \bar{X})^T ] 其中，( n ) 为样本数，( \bar{X} ) 为特征均值。

    特征值分解：
        对协方差矩阵进行特征值分解，得到特征值和特征向量。特征值表示主成分的重要性，特征向量表示主成分的方向。

    选择主成分：
        根据特征值的大小选择主成分。通常选择累计方差贡献率达到95%或99%的主成分。

    转换数据：
        将原始数据投影到选择的主成分上，得到降维后的数据。转换公式为： [ X_{new} = X_{standard} \cdot W ] 其中，( W ) 为选择的特征向量矩阵。

PCA的优点

    降维效果显著：PCA能够在保证数据主要信息不丢失的前提下，显著减少数据维度，降低计算复杂度。
    消除多重共线性：PCA可以有效消除特征之间的多重共线性，提高模型的稳定性和预测性能。
    提高可视化效果：降维后的数据可以更方便地进行可视化，便于理解和分析。

PCA的应用场景

    数据预处理：PCA常用于数据预处理阶段，减少数据维度，去除噪声，提高模型训练效率。
    特征提取：在计算机视觉、图像处理等领域，通过PCA提取图像的主要特征，减少计算量。
    数据压缩：在数据存储和传输中，通过PCA进行数据压缩，减少存储空间和传输带宽。

实验结果中的PCA应用

在本文的实验中，我们使用PCA将IRIS数据集降维到两个主成分，并进行KNN分类。实验结果表明，PCA降维能够在保持较高分类准确率（95.182%）的同时，显著减少训练和预测时间（平均运行总时间为0.4288646492毫秒）。这表明PCA是一种有效的降维策略，可以在提升KNN算法效率的同时，保证分类性能。

综上所述，主成分分析（PCA）作为一种重要的降维技术，通过减少数据维度，保留主要信息，不仅提高了KNN算法的效率，还在很多实际应用中展现了其强大的能力。未来的研究可以结合其他降维技术和特征选择方法，进一步提升KNN算法的效率和性能。

## 结论

本文通过不同的特征组合和降维策略，探讨了如何在保证分类性能的前提下提升KNN算法的效率。实验结果表明，PCA降维是一种有效的策略，可以在保持较高分类准确率的同时，显著减少训练和预测时间。未来的研究可以进一步探讨其他降维技术和特征选择方法，以寻求更优的KNN算法效率提升策略。
#### 参考文献

    Guo, G., Wang, H., Bell, D., Bi, Y., & Greer, K. (2004). KNN model-based approach in classification. In OTM Confederated International Conferences" On the Move to Meaningful Internet Systems" (pp. 986-996). Springer, Berlin, Heidelberg.
    Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1), 21-27.
    Jain, A. K., Duin, R. P. W., & Mao, J. (2000). Statistical pattern recognition: A review. IEEE Transactions on pattern analysis and machine intelligence, 22(1), 4-37.
